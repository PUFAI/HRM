batch_size: 2
cycles: 8
d_model: 768
devices: 1
disable_act: false
lr: 0.0003
max_epochs: 1
micro_steps: 4
n_heads: 12
num_high_blocks: 2
num_low_blocks: 2
num_workers: 0
precision: bf16
seq_len: 512
strategy: auto
stream_repo: HuggingFaceFW/fineweb
stream_split_train: train
stream_split_val: null
tokenizer_name: null
tokenizer_path: ../scripts/tokenizer_sp/tokenizer.model
total_steps: 10000
train_dir: null
val_dir: null
val_limit: 1024
vocab_size: 32000
warmup_steps: 2000
weight_decay: 0.1
